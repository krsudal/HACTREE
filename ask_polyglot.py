# %%
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from langchain_community.vectorstores import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings
# import time

# # âœ… ì„¤ì •
# VECTOR_DB_PATH = "vector_db/export_faiss"
# EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
# LLM_MODEL_NAME = "EleutherAI/polyglot-ko-1.3b"

# # âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
# DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"
# print(f"[ğŸ“¡] Using device: {DEVICE}")

# # âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# # âœ… FAISS DB ë¡œë“œ
# db = FAISS.load_local(VECTOR_DB_PATH, embedding_model, allow_dangerous_deserialization=True)

# # âœ… LLM ë¡œë“œ
# tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
# model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME, torch_dtype=torch.float16)
# model.to(DEVICE)

# # âœ… ì§ˆë¬¸ ì…ë ¥
# question = input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# # âœ… ê²€ìƒ‰
# start_time = time.time()
# docs = db.similarity_search(question, k=3)
# retrieved_context = "\n\n".join([doc.page_content for doc in docs])
# print("\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½:")
# print(retrieved_context[:500] + ("..." if len(retrieved_context) > 500 else ""))

# # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
# prompt = f"""
# ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤.

# ë¬¸ì„œ:
# {retrieved_context}

# ì§ˆë¬¸: {question}
# ë‹µë³€:
# """

# # âœ… í† í°í™” ë° ìƒì„±
# inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
# inputs.pop("token_type_ids", None)  # <-- ì´ ì¤„ ì¶”ê°€
# inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

# with torch.no_grad():
#     outputs = model.generate(
#         **inputs,
#         max_new_tokens=256,
#         do_sample=True,
#         top_k=50,
#         top_p=0.95,
#         temperature=0.7,
#     )

# print("\nğŸ§  Polyglot-ko ì‘ë‹µ:")
# print(tokenizer.decode(outputs[0], skip_special_tokens=True))
# print(f"\nâ±ï¸ ì‘ë‹µ ìƒì„± ì‹œê°„: {round(time.time() - start_time, 2)}ì´ˆ")

# %%
# !pip install tf-keras

# %%
#ask_polyglot
# ask_polyglot.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ğŸ”§ ì„¤ì •
LLM_MODEL_NAME = "EleutherAI/polyglot-ko-1.3b"
VECTOR_DB_PATH = "vector_db/export_faiss"
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"

# ğŸ§  ëª¨ë¸ ë¡œë“œ
print(f"ğŸ’» Using device: {DEVICE}")
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL_NAME,
    torch_dtype=torch.float16 if DEVICE != "cpu" else torch.float32
)
model.to(DEVICE)

# ğŸ§  ë²¡í„° DB ë° ì„ë² ë”© ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME,
                                        model_kwargs={"device": DEVICE})
db = FAISS.load_local(VECTOR_DB_PATH, embedding_model, allow_dangerous_deserialization=True)

# â“ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜
def answer_question_from_vectorstore(question: str) -> str:
    # 1. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    docs = db.similarity_search(question, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"{context}\n\nì§ˆë¬¸: {question}\në‹µë³€:"

    # 3. í† í¬ë‚˜ì´ì¦ˆ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)

    # 4. token_type_ids ì œê±° (GPTë¥˜ ëª¨ë¸ ë¹„í˜¸í™˜)
    if 'token_type_ids' in inputs:
        del inputs['token_type_ids']

    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    # 5. ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
        )

    # 6. ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ì„ íƒ: ì§ˆë¬¸ ì´í›„ë§Œ ì¶”ì¶œí•˜ê±°ë‚˜ "ë‹µë³€:" ì´í›„ë§Œ ì¶”ì¶œ (ê°„ë‹¨í•œ ì •ì œ)
    if "ë‹µë³€:" in response:
        response = response.split("ë‹µë³€:")[-1].strip()
    return response

# %%
