#%%
# tariff_barrier.py

# import torch
# from langchain_community.vectorstores import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings
# from transformers import AutoTokenizer, AutoModelForCausalLM

# # ì„¤ì •
# LLM_MODEL_NAME = "EleutherAI/polyglot-ko-1.3b"
# VECTOR_DB_PATH = "vector_db/tariff_japan_cosmetics"
# EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
# DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"

# # ëª¨ë¸ ë¡œë“œ
# print(f"ğŸ’» ê´€ì„¸/TBT QA: Using device: {DEVICE}")
# tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
# model = AutoModelForCausalLM.from_pretrained(
#     LLM_MODEL_NAME,
#     torch_dtype=torch.float16 if DEVICE != "cpu" else torch.float32
# ).to(DEVICE)

# # ì„ë² ë”© ë° ë²¡í„° DB ë¡œë“œ
# embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME,
#                                         model_kwargs={"device": DEVICE})

# try:
#     db = FAISS.load_local(VECTOR_DB_PATH, embedding_model, allow_dangerous_deserialization=True)
# except Exception as e:
#     raise RuntimeError(f"âŒ ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {e}")

# # ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜
# def answer_tariff_barrier_question(question: str) -> str:
#     docs = db.similarity_search(question, k=3)
#     context = "\n".join([doc.page_content for doc in docs])

#     prompt = f"{context}\n\nì§ˆë¬¸: {question}\në‹µë³€:"

#     inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
#     if 'token_type_ids' in inputs:
#         del inputs['token_type_ids']
#     inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=True,
#             temperature=0.7,
#             top_k=50,
#             top_p=0.95,
#         )

#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     if "ë‹µë³€:" in response:
#         response = response.split("ë‹µë³€:")[-1].strip()
#     return response


# %%
import os
import pickle
import torch
import faiss
from typing import List
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# ì„¤ì •
LLM_MODEL_NAME = "EleutherAI/polyglot-ko-1.3b"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
VECTOR_DB_ROOT = "vector_db/hs_based"
DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"

print(f"ğŸ’» ì „ëµ QA: Using device: {DEVICE}")

# LLM & ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL_NAME,
    torch_dtype=torch.float16 if DEVICE != "cpu" else torch.float32
).to(DEVICE)

embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

# âœ… ì „ëµ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜ (HS ì½”ë“œ + êµ­ê°€ ê¸°ë°˜)
def answer_tariff_barrier_question(hs_code: str, country: str, question: str, top_k: int = 3) -> str:
    db_path = os.path.join(VECTOR_DB_ROOT, country, hs_code)
    index_path = os.path.join(db_path, "index.faiss")
    chunks_path = os.path.join(db_path, "chunks.pkl")

    if not os.path.exists(index_path) or not os.path.exists(chunks_path):
        return f"[!] '{country}/{hs_code}' ë²¡í„° DB ë˜ëŠ” chunk ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # 1. ë²¡í„° DB ë¡œë“œ
    index = faiss.read_index(index_path)
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)

    # 2. ì§ˆë¬¸ ì„ë² ë”©
    query_vector = embedding_model.encode([question])
    distances, indices = index.search(query_vector, top_k)

    # 3. ê´€ë ¨ chunk í…ìŠ¤íŠ¸ ì¶”ì¶œ
    selected_chunks = [chunks[i]["text"] for i in indices[0] if i < len(chunks)]
    context = "\n\n".join(selected_chunks)

    # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"{context}\n\nì§ˆë¬¸: {question}\në‹µë³€:"

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    # 5. LLM ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "ë‹µë³€:" in response:
        response = response.split("ë‹µë³€:")[-1].strip()
    return response
